{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":20270,"databundleVersionId":1222630,"sourceType":"competition"},{"sourceId":1243687,"sourceType":"datasetVersion","datasetId":690737}],"dockerImageVersionId":29928,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Melanoma starter pipeline by [@shonenkov](https://www.kaggle.com/shonenkov)","metadata":{}},{"cell_type":"markdown","source":"# Main ideas\n\n\n- Using External Data\n- StratifyGroupKFold\n- Focal Loss / Label Smoothing\n- BalanceClassSampler\n- SimpleAugs\n- 512x512 image size\n- EfficientNet","metadata":{}},{"cell_type":"markdown","source":"# Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install -q efficientnet_pytorch > /dev/null","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-30T04:22:10.178162Z","iopub.execute_input":"2024-09-30T04:22:10.178608Z","iopub.status.idle":"2024-09-30T04:22:17.090583Z","shell.execute_reply.started":"2024-09-30T04:22:10.178566Z","shell.execute_reply":"2024-09-30T04:22:17.089520Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: You are using pip version 20.1; however, version 24.0 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from glob import glob\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport cv2\nfrom skimage import io\nimport albumentations as A\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.nn import functional as F\nfrom glob import glob\nimport sklearn\nfrom torch import nn\nimport warnings\n\nwarnings.filterwarnings(\"ignore\") \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\nSEED = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-30T04:22:17.092864Z","iopub.execute_input":"2024-09-30T04:22:17.093204Z","iopub.status.idle":"2024-09-30T04:22:17.107262Z","shell.execute_reply.started":"2024-09-30T04:22:17.093164Z","shell.execute_reply":"2024-09-30T04:22:17.106408Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"# External data\n\nI have prepared kernel with merging data. Don't forget to read [this kernel](https://www.kaggle.com/shonenkov/merge-external-data) ;)","metadata":{}},{"cell_type":"code","source":"DATA_PATH = '../input/melanoma-merged-external-data-512x512-jpeg'","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:17.109035Z","iopub.execute_input":"2024-09-30T04:22:17.109454Z","iopub.status.idle":"2024-09-30T04:22:17.123942Z","shell.execute_reply.started":"2024-09-30T04:22:17.109411Z","shell.execute_reply":"2024-09-30T04:22:17.123223Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"# StratifyGroupKFold\n\nI think group by patient_id is very important. Also I think that stratify by sex, target, source, anatom_site_general_challenge also useful.\nCode with getting folds you can find [here](https://www.kaggle.com/shonenkov/merge-external-data)","metadata":{}},{"cell_type":"code","source":"df_folds = pd.read_csv(f'{DATA_PATH}/folds.csv', index_col='image_id')","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:17.125455Z","iopub.execute_input":"2024-09-30T04:22:17.125820Z","iopub.status.idle":"2024-09-30T04:22:17.273408Z","shell.execute_reply.started":"2024-09-30T04:22:17.125789Z","shell.execute_reply":"2024-09-30T04:22:17.272663Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"set(df_folds[df_folds['fold'] == 0]['patient_id'].values).intersection(df_folds[df_folds['fold'] == 1]['patient_id'].values)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:17.277599Z","iopub.execute_input":"2024-09-30T04:22:17.277902Z","iopub.status.idle":"2024-09-30T04:22:17.294327Z","shell.execute_reply.started":"2024-09-30T04:22:17.277874Z","shell.execute_reply":"2024-09-30T04:22:17.293507Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"set()"},"metadata":{}}]},{"cell_type":"code","source":"df_folds[df_folds['fold'] == 0]['target'].hist();","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:17.297253Z","iopub.execute_input":"2024-09-30T04:22:17.297621Z","iopub.status.idle":"2024-09-30T04:22:17.467624Z","shell.execute_reply.started":"2024-09-30T04:22:17.297591Z","shell.execute_reply":"2024-09-30T04:22:17.466681Z"},"trusted":true},"execution_count":54,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"df_folds[df_folds['fold'] == 1]['target'].hist();","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:17.469200Z","iopub.execute_input":"2024-09-30T04:22:17.469588Z","iopub.status.idle":"2024-09-30T04:22:17.639459Z","shell.execute_reply.started":"2024-09-30T04:22:17.469555Z","shell.execute_reply":"2024-09-30T04:22:17.638607Z"},"trusted":true},"execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"# Augmentations","metadata":{}},{"cell_type":"code","source":"# def get_train_transforms():\n#     return A.Compose([\n#             A.RandomSizedCrop(min_max_height=(400, 400), height=512, width=512, p=0.5),\n#             A.RandomRotate90(p=0.5),\n#             A.HorizontalFlip(p=0.5),\n#             A.VerticalFlip(p=0.5),\n#             A.Resize(height=512, width=512, p=1),\n#             A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n#             ToTensorV2(p=1.0),                  \n#         ], p=1.0)\n\n# def get_valid_transforms():\n#     return A.Compose([\n#             A.Resize(height=512, width=512, p=1.0),\n#             ToTensorV2(p=1.0),\n#         ], p=1.0)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:17.640529Z","iopub.execute_input":"2024-09-30T04:22:17.640830Z","iopub.status.idle":"2024-09-30T04:22:17.644537Z","shell.execute_reply.started":"2024-09-30T04:22:17.640801Z","shell.execute_reply":"2024-09-30T04:22:17.643665Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"image_size=512\ndef get_train_transforms():\n    return A.Compose([\n           A.Transpose(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.5),  # Adjust brightness and contrast\n#     A.RandomContrast(limit=0.2, p=0.75),\n    A.OneOf([\n        A.MotionBlur(blur_limit=5),\n        A.MedianBlur(blur_limit=5),\n        A.GaussianBlur(blur_limit=5),\n        A.GaussNoise(var_limit=(5.0, 30.0)),\n    ], p=0.7),\n\n    A.OneOf([\n        A.OpticalDistortion(distort_limit=1.0),\n        A.GridDistortion(num_steps=5, distort_limit=1.),\n        A.ElasticTransform(alpha=3),\n    ], p=0.7),\n\n    A.CLAHE(clip_limit=4.0, p=0.7),\n    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n    A.Resize(image_size, image_size),\n    A.Cutout(max_h_size=int(image_size * 0.375), max_w_size=int(image_size * 0.375), num_holes=1, p=0.7),    \n    A.Normalize(),\n            ToTensorV2(p=1.0),                  \n        ])\n\ndef get_valid_transforms():\n    return A.Compose([\n           A.Resize(image_size, image_size),\n    A.Normalize()\n        ])","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:17.645982Z","iopub.execute_input":"2024-09-30T04:22:17.646303Z","iopub.status.idle":"2024-09-30T04:22:17.663235Z","shell.execute_reply.started":"2024-09-30T04:22:17.646263Z","shell.execute_reply":"2024-09-30T04:22:17.662375Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"TRAIN_ROOT_PATH = f'{DATA_PATH}/512x512-dataset-melanoma/512x512-dataset-melanoma'\n\ndef onehot(size, target):\n    vec = torch.zeros(size, dtype=torch.float32)\n    vec[target] = 1.\n    return vec\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self,df: pd.DataFrame, image_ids, labels, transforms=None,meta_features=None):\n        super().__init__()\n        self.image_ids = image_ids\n        self.labels = labels\n        self.transforms = transforms\n        self.df = df\n#         self.train = train\n        self.meta_features = meta_features\n        \n\n    def __getitem__(self, idx: int):\n        image_id = self.image_ids[idx]\n        image = cv2.imread(f'{TRAIN_ROOT_PATH}/{image_id}.jpg', cv2.IMREAD_COLOR)\n        \n\n        label = self.labels[idx]\n\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n#             image = image.astype(np.float32) / 255.0\n\n        target = onehot(2, label)\n        \n        \n        meta = np.array(self.df.loc[self.df['image_id'] == image_id][self.meta_features].values, dtype=np.float32)\n#         print(meta.shape)\n        \n        return  (image, meta)  , target\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\n    def get_labels(self):\n        return list(self.labels)\n    \nclass MelanomaDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, imfolder: str, isic_imfolder: str = None, train: bool = True, transforms=None, meta_features=None):\n        \"\"\"\n        Class initialization\n        Args:\n            df (pd.DataFrame): DataFrame with data description\n            imfolder (str): folder with images\n            isic_imfolder (str): optional folder with ISIC images\n            train (bool): flag of whether a training dataset is being initialized or testing one\n            transforms: image transformation method to be applied\n            meta_features (list): list of features with meta information, such as sex and age\n        \"\"\"\n        self.df = df\n        self.imfolder = imfolder\n        self.isic_imfolder = isic_imfolder  # New parameter for ISIC images\n        self.transforms = transforms\n        self.train = train\n        self.meta_features = meta_features\n        \n    def get_image_path(self, image_name):\n        \"\"\"\n        Helper function to get the full image path.\n        Checks the main image folder first, then the ISIC folder if specified.\n        \"\"\"\n        # Construct main image path\n        main_path = os.path.join(self.imfolder, f\"{image_name}.jpg\")\n        if os.path.exists(main_path):\n            return main_path\n        \n        # If ISIC images are needed, check in the ISIC folder\n        if False:\n            isic_path = os.path.join(self.isic_imfolder, f\"{image_name}.jpg\")\n            if os.path.exists(isic_path):\n                return isic_path\n        \n        # Raise an error if the image is not found in either location\n        raise FileNotFoundError(f\"Image {image_name} not found in both folders.\")\n\n    def __getitem__(self, index):\n        # Get the image name from the DataFrame\n        image_name = self.df.iloc[index]['image_id']\n        \n        # Use the helper function to get the full image path\n        im_path = self.get_image_path(image_name)\n        \n        # Load the image\n        x = cv2.imread(im_path)\n        x = x.astype(np.float32) / 255.0\n\n        \n        # Extract meta features\n        meta = np.array(self.df.iloc[index][self.meta_features].values, dtype=np.float32)\n        print(meta.shape)\n        # Convert image to RGB\n#         image = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n\n        # Apply transformations if any\n        if self.transforms is not None:\n            res = self.transforms(image=x)\n            x = res['image']\n        else:\n            x = image.astype(np.float32)\n\n        # Change channel order from HWC to CHW\n#         x = x.transpose(2, 0, 1)\n            \n        # Return the tuple of (image, meta) and label if in training mode\n        if self.train:\n            y = self.df.iloc[index]['target']\n            return (x, meta), y\n        else:\n            return (x, meta)\n    \n    def __len__(self):\n        return len(self.df)\n    ","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-30T04:22:17.664747Z","iopub.execute_input":"2024-09-30T04:22:17.665212Z","iopub.status.idle":"2024-09-30T04:22:17.694685Z","shell.execute_reply.started":"2024-09-30T04:22:17.665170Z","shell.execute_reply":"2024-09-30T04:22:17.693543Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass RocAucMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.y_true = np.array([0,1])\n        self.y_pred = np.array([0.5,0.5])\n        self.score = 0\n\n    def update(self, y_true, y_pred):\n        y_true = y_true.cpu().numpy().argmax(axis=1).clip(min=0, max=1).astype(int)\n        # y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n        y_pred = nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,1]\n        self.y_true = np.hstack((self.y_true, y_true))\n        self.y_pred = np.hstack((self.y_pred, y_pred))\n        self.score = sklearn.metrics.roc_auc_score(self.y_true, self.y_pred)\n\n    @property\n    def avg(self):\n        return self.score\n\n    \nclass APScoreMeter(RocAucMeter):\n    def __init__(self):\n        super(APScoreMeter, self).__init__()\n\n    def update(self, y_true, y_pred):\n        y_true = y_true.cpu().numpy().argmax(axis=1).clip(min=0, max=1).astype(int)\n        y_pred = nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,1]\n        self.y_true = np.hstack((self.y_true, y_true))\n        self.y_pred = np.hstack((self.y_pred, y_pred))\n        self.score = sklearn.metrics.average_precision_score(self.y_true, self.y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:17.696240Z","iopub.execute_input":"2024-09-30T04:22:17.696625Z","iopub.status.idle":"2024-09-30T04:22:17.717347Z","shell.execute_reply.started":"2024-09-30T04:22:17.696586Z","shell.execute_reply":"2024-09-30T04:22:17.716405Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"# Loss","metadata":{}},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):\n        if self.logits:\n            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets)\n        else:\n            BCE_loss = F.binary_cross_entropy(inputs, targets)\n\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss\n        \nclass LabelSmoothing(nn.Module):\n    def __init__(self, smoothing = 0.1):\n        super(LabelSmoothing, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n\n    def forward(self, x, target):\n        if self.training:\n            x = x.float()\n            target = target.float()\n            logprobs = torch.nn.functional.log_softmax(x, dim = -1)\n\n            nll_loss = -logprobs * target\n            nll_loss = nll_loss.sum(-1)\n    \n            smooth_loss = -logprobs.mean(dim=-1)\n\n            loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n\n            return loss.mean()\n        else:\n            return torch.nn.functional.cross_entropy(x, target)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:17.718778Z","iopub.execute_input":"2024-09-30T04:22:17.719085Z","iopub.status.idle":"2024-09-30T04:22:17.734064Z","shell.execute_reply.started":"2024-09-30T04:22:17.719056Z","shell.execute_reply":"2024-09-30T04:22:17.733198Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"# Net","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# from efficientnet_pytorch import EfficientNet\n\n# def get_net():\n#     net = EfficientNet.from_pretrained('efficientnet-b5')\n#     net._fc = nn.Linear(in_features=2048, out_features=2, bias=True)\n#     return net\n\n# net = get_net().cuda()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-30T04:22:17.735381Z","iopub.execute_input":"2024-09-30T04:22:17.735778Z","iopub.status.idle":"2024-09-30T04:22:17.747558Z","shell.execute_reply.started":"2024-09-30T04:22:17.735735Z","shell.execute_reply":"2024-09-30T04:22:17.746827Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/melanoma-merged-external-data-512x512-jpeg/folds.csv')\ntest_df = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/test.csv')\n# isic_train_df = pd.read_csv('/kaggle/input/jpeg-isic2019-512x512//train.csv')\n\n# Concatenate the two datasets for training\n# train_df = pd.concat([train_df, isic_train_df], axis=0, ignore_index=True)\n\n# One-hot encoding of anatom_site_general_challenge feature\nconcat = pd.concat([train_df['anatom_site_general_challenge'], test_df['anatom_site_general_challenge']], ignore_index=True)\ndummies = pd.get_dummies(concat, dummy_na=True, dtype=np.uint8, prefix='site')\ntrain_df = pd.concat([train_df, dummies.iloc[:train_df.shape[0]]], axis=1)\ntest_df = pd.concat([test_df, dummies.iloc[train_df.shape[0]:].reset_index(drop=True)], axis=1)\n\n# Sex features\ntrain_df['sex'] = train_df['sex'].map({'male': 1, 'female': 0})\ntest_df['sex'] = test_df['sex'].map({'male': 1, 'female': 0})\ntrain_df['sex'] = train_df['sex'].fillna(-1)\ntest_df['sex'] = test_df['sex'].fillna(-1)\n\n# Age features\ntrain_df['age_approx'] /= train_df['age_approx'].max()\ntest_df['age_approx'] /= test_df['age_approx'].max()\ntrain_df['age_approx'] = train_df['age_approx'].fillna(0)\ntest_df['age_approx'] = test_df['age_approx'].fillna(0)\n\ntrain_df['patient_id'] = train_df['patient_id'].fillna(0)\nmeta_features = ['sex', 'age_approx'] + [col for col in train_df.columns if 'site_' in col]\nmeta_features.remove('anatom_site_general_challenge')","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:17.748663Z","iopub.execute_input":"2024-09-30T04:22:17.748979Z","iopub.status.idle":"2024-09-30T04:22:17.925422Z","shell.execute_reply.started":"2024-09-30T04:22:17.748951Z","shell.execute_reply":"2024-09-30T04:22:17.924701Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self, arch, n_meta_features: int):\n        super(Net, self).__init__()\n        self.arch = arch\n        self.dropouts = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n        if 'ResNet' in str(arch.__class__):\n            self.arch.fc = nn.Linear(in_features=512, out_features=500, bias=True)\n        elif 'EfficientNet' in str(arch.__class__):\n            \n            \n            \n#             net = EfficientNet.from_pretrained('efficientnet-b5')\n#             net._fc = nn.Linear(in_features=2048, out_features=2, bias=True)\n            self.arch._fc = nn.Linear(in_features=2048, out_features=500, bias=True)\n        elif 'Xception' in str(arch.__class__):\n            self.arch.fc = nn.Linear(in_features=2048, out_features=500, bias=True)  # Xception typically has 2048 features\n        \n        self.meta = nn.Sequential(nn.Linear(n_meta_features, 500),\n                                  nn.BatchNorm1d(500),\n                                  nn.ReLU(),\n                                  nn.Dropout(p=0.2),\n                                  nn.Linear(500, 250),  # FC layer output will have 250 features\n                                  nn.BatchNorm1d(250),\n                                  nn.ReLU(),\n                                  nn.Dropout(p=0.2))\n        self.ouput = nn.Linear(500 + 250, 2)\n        \n    def forward(self, inputs):\n        \"\"\"\n        No sigmoid in forward because we are going to use BCEWithLogitsLoss\n        Which applies sigmoid for us when calculating a loss\n        \"\"\"\n        x, meta = inputs\n#         print(meta.shape)\n        \n        \n        cnn_features = self.arch(x)\n#         print(cnn_features.shape)\n        meta_features = self.meta(meta.squeeze())\n#         print(meta_features.shape)\n        features = torch.cat((cnn_features, meta_features), dim=1)\n#         for i, dropout in enumerate(self.dropouts):\n#             if i == 0:\n#                 out = self.ouput(dropout(x))\n#             else:\n#                 out += self.ouput(dropout(x))\n        output = self.ouput(features)\n#         out /= len(self.dropouts)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:17.926756Z","iopub.execute_input":"2024-09-30T04:22:17.927135Z","iopub.status.idle":"2024-09-30T04:22:17.942227Z","shell.execute_reply.started":"2024-09-30T04:22:17.927096Z","shell.execute_reply":"2024-09-30T04:22:17.941188Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"## efficientnet-b5 :0.92","metadata":{}},{"cell_type":"code","source":"\nfrom efficientnet_pytorch import EfficientNet\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\narch = EfficientNet.from_pretrained('efficientnet-b5')\nnet = Net(arch=arch, n_meta_features=len(meta_features))  # Initialize model\nnet = net.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:17.943366Z","iopub.execute_input":"2024-09-30T04:22:17.943627Z","iopub.status.idle":"2024-09-30T04:22:18.613337Z","shell.execute_reply.started":"2024-09-30T04:22:17.943600Z","shell.execute_reply":"2024-09-30T04:22:18.612494Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b5\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Xception?","metadata":{}},{"cell_type":"code","source":"# !pip install timm\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:18.614773Z","iopub.execute_input":"2024-09-30T04:22:18.615061Z","iopub.status.idle":"2024-09-30T04:22:18.618711Z","shell.execute_reply.started":"2024-09-30T04:22:18.615032Z","shell.execute_reply":"2024-09-30T04:22:18.617944Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# !pip show torch\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:18.619869Z","iopub.execute_input":"2024-09-30T04:22:18.620178Z","iopub.status.idle":"2024-09-30T04:22:18.630501Z","shell.execute_reply.started":"2024-09-30T04:22:18.620148Z","shell.execute_reply":"2024-09-30T04:22:18.629667Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# import timm  # Import timm for loading Xception\n\n# arch = timm.create_model('xception', pretrained=True)\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# net = Net(arch=arch, n_meta_features=len(meta_features))  # Initialize model\n# net = net.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:18.631676Z","iopub.execute_input":"2024-09-30T04:22:18.632024Z","iopub.status.idle":"2024-09-30T04:22:18.643145Z","shell.execute_reply.started":"2024-09-30T04:22:18.631992Z","shell.execute_reply":"2024-09-30T04:22:18.642206Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"# Fitter","metadata":{}},{"cell_type":"code","source":"class Fitter:\n    \n    def __init__(self, model, device, config, folder):\n        self.config = config\n        self.epoch = 0\n\n        self.base_dir = f'./{folder}'\n        if not os.path.exists(self.base_dir):\n            os.makedirs(self.base_dir)\n\n        self.log_path = f'{self.base_dir}/log.txt'\n        self.best_score = 0\n        self.best_loss = 10**5\n        self.best_ap = 0\n        \n        self.model = model\n        self.device = device\n\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ] \n\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n\n#         self.criterion = FocalLoss(logits=True).to(self.device)\n        self.criterion = LabelSmoothing().to(self.device)\n        self.log(f'Fitter prepared. Device is {self.device}')\n\n    def fit(self, train_loader, validation_loader):\n        for e in range(self.config.n_epochs):\n            if self.config.verbose:\n                lr = self.optimizer.param_groups[0]['lr']\n                timestamp = datetime.utcnow().isoformat()\n                self.log(f'\\n{timestamp}\\nLR: {lr}')\n\n            t = time.time()\n            summary_loss, roc_auc_scores, ap_scores = self.train_one_epoch(train_loader)\n            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, roc_auc: {roc_auc_scores.avg:.5f}, ap: {ap_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n\n            t = time.time()\n            summary_loss, roc_auc_scores, ap_scores = self.validation(validation_loader)\n\n            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, roc_auc: {roc_auc_scores.avg:.5f}, ap: {ap_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n            if summary_loss.avg < self.best_loss:\n                self.best_loss = summary_loss.avg\n                self.save_model(f'{self.base_dir}/best-loss-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n                for path in sorted(glob(f'{self.base_dir}/best-loss-checkpoint-*epoch.bin'))[:-2]:\n                    os.remove(path)\n                    \n            if roc_auc_scores.avg > self.best_score:\n                self.best_score = roc_auc_scores.avg\n                self.save_model(f'{self.base_dir}/best-score-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n                for path in sorted(glob(f'{self.base_dir}/best-score-checkpoint-*epoch.bin'))[:-2]:\n                    os.remove(path)\n                    \n            if ap_scores.avg > self.best_ap:\n                self.best_ap = ap_scores.avg\n                self.save_model(f'{self.base_dir}/best-ap-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n                for path in sorted(glob(f'{self.base_dir}/best-ap-checkpoint-*epoch.bin'))[:-2]:\n                    os.remove(path)\n\n            if self.config.validation_scheduler:\n                self.scheduler.step(metrics=summary_loss.avg)\n\n            self.epoch += 1\n\n    def validation(self, val_loader):\n        self.model.eval()\n        summary_loss = AverageMeter()\n        roc_auc_scores = RocAucMeter()\n        ap_scores = APScoreMeter()\n        t = time.time()\n        for step, (data, targets) in enumerate(val_loader):\n            images=data[0]\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Val Step {step}/{len(val_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, roc_auc: {roc_auc_scores.avg:.5f}, ap: {ap_scores.avg:.5f} ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            with torch.no_grad():\n                targets = targets.to(self.device).float()\n#                 images=\n                batch_size = images.shape[0]\n                data[0] = data[0].to(self.device).float()\n                data[1] = data[1].to(self.device).float()\n\n                \n#                 =images\n                outputs = self.model(data)\n                loss = self.criterion(outputs, targets)\n                roc_auc_scores.update(targets, outputs)\n                ap_scores.update(targets, outputs)\n                summary_loss.update(loss.detach().item(), batch_size)\n\n        return summary_loss, roc_auc_scores, ap_scores\n\n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        summary_loss = AverageMeter()\n        roc_auc_scores = RocAucMeter()\n        ap_scores = APScoreMeter()\n        t = time.time()\n        for step, (data, targets) in enumerate(train_loader):\n            images=data[0]\n\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Train Step {step}/{len(train_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, roc_auc: {roc_auc_scores.avg:.5f}, ap: {ap_scores.avg:.5f} ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            \n            targets = targets.to(self.device).float()\n            data[0] = data[0].to(self.device).float()\n            data[1] = data[1].to(self.device).float()            \n            batch_size = images.shape[0]\n            \n            self.optimizer.zero_grad()\n            outputs = self.model(data)\n            loss = self.criterion(outputs, targets)\n            loss.backward()\n            \n            roc_auc_scores.update(targets, outputs)\n            ap_scores.update(targets, outputs)\n            summary_loss.update(loss.detach().item(), batch_size)\n\n            self.optimizer.step()\n\n            if self.config.step_scheduler:\n                self.scheduler.step()\n\n        return summary_loss, roc_auc_scores, ap_scores\n    \n    def save_model(self, path):\n        self.model.eval()\n        torch.save(self.model.state_dict(),path)\n\n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_score': self.best_score,\n            'best_ap': self.best_ap,\n            'best_loss': self.best_loss,\n            'epoch': self.epoch,\n        }, path)\n\n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_score = checkpoint['best_score']\n        self.best_ap = checkpoint['best_ap']\n        self.best_loss = checkpoint['best_loss']\n        self.epoch = checkpoint['epoch']\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:18.644350Z","iopub.execute_input":"2024-09-30T04:22:18.644651Z","iopub.status.idle":"2024-09-30T04:22:18.699143Z","shell.execute_reply.started":"2024-09-30T04:22:18.644621Z","shell.execute_reply":"2024-09-30T04:22:18.698089Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"class TrainGlobalConfig:\n    num_workers = 2\n    batch_size = 8 \n    n_epochs = 20\n    lr = 0.00003\n\n    # -------------------\n    verbose = True\n    verbose_step = 1\n    # -------------------\n\n    # --------------------\n    step_scheduler = False  # do scheduler.step after optimizer.step\n    validation_scheduler = True  # do scheduler.step after validation stage loss\n\n#     SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n#     scheduler_params = dict(\n#         max_lr=0.001,\n#         epochs=n_epochs,\n#         steps_per_epoch=int(len(train_dataset) / batch_size),\n#         pct_start=0.1,\n#         anneal_strategy='cos', \n#         final_div_factor=10**5\n#     )\n    \n    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n    scheduler_params = dict(\n        mode='min',\n        factor=0.8,\n        patience=1,\n        verbose=False, \n        threshold=0.0001,\n        threshold_mode='abs',\n        cooldown=0, \n        min_lr=1e-8,\n        eps=1e-08\n    )\n    # --------------------","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:18.700427Z","iopub.execute_input":"2024-09-30T04:22:18.700732Z","iopub.status.idle":"2024-09-30T04:22:18.711968Z","shell.execute_reply.started":"2024-09-30T04:22:18.700688Z","shell.execute_reply":"2024-09-30T04:22:18.711166Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"# Save all states for \"honest\" training of folds","metadata":{}},{"cell_type":"code","source":"fitter = Fitter(model=net, device=torch.device('cuda:0'), config=TrainGlobalConfig, folder='base_state')\nBASE_STATE_PATH = f'{fitter.base_dir}/base_state.bin'\nfitter.save(BASE_STATE_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:18.713213Z","iopub.execute_input":"2024-09-30T04:22:18.713523Z","iopub.status.idle":"2024-09-30T04:22:19.105159Z","shell.execute_reply.started":"2024-09-30T04:22:18.713491Z","shell.execute_reply":"2024-09-30T04:22:19.104014Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Fitter prepared. Device is cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"from catalyst.data.sampler import BalanceClassSampler\n\ndef train_fold(fold_number):\n    \n    \n    train_dataset = DatasetRetriever(\n        df=train_df,\n        \n        image_ids=df_folds[df_folds['fold'] != fold_number].index.values,\n        labels=df_folds[df_folds['fold'] != fold_number].target.values,\n        transforms=get_train_transforms(),\n        meta_features=meta_features\n\n    )\n\n    df_val = df_folds[(df_folds['fold'] == fold_number) & (df_folds['source'] == 'ISIC20')]\n\n    validation_dataset = DatasetRetriever(\n        df=train_df,\n        image_ids=df_val.index.values,\n        labels=df_val.target.values,\n        transforms=get_valid_transforms(),\n        meta_features=meta_features\n\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        sampler=BalanceClassSampler(labels=train_dataset.get_labels(), mode=\"downsampling\"),\n        batch_size=TrainGlobalConfig.batch_size,\n        pin_memory=False,\n        drop_last=True,\n        num_workers=TrainGlobalConfig.num_workers,\n    )\n    val_loader = torch.utils.data.DataLoader(\n        validation_dataset, \n        batch_size=TrainGlobalConfig.batch_size,\n        num_workers=TrainGlobalConfig.num_workers,\n        shuffle=False,\n        sampler=SequentialSampler(validation_dataset),\n        pin_memory=False,\n    )\n\n    fitter = Fitter(model=net, device=torch.device('cuda:0'), config=TrainGlobalConfig, folder=f'fold{fold_number}')\n    fitter.load(BASE_STATE_PATH)\n    fitter.fit(train_loader, val_loader)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:19.106517Z","iopub.execute_input":"2024-09-30T04:22:19.106840Z","iopub.status.idle":"2024-09-30T04:22:19.119656Z","shell.execute_reply.started":"2024-09-30T04:22:19.106808Z","shell.execute_reply":"2024-09-30T04:22:19.118799Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\nfor fold_number in range(1): # range(5)\n    train_fold(fold_number=fold_number)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T04:22:19.120841Z","iopub.execute_input":"2024-09-30T04:22:19.121158Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Fitter prepared. Device is cuda:0\n\n2024-09-30T04:22:19.437612\nLR: 3e-05\nTrain Step 63/1089, summary_loss: 0.66009, roc_auc: 0.67300, ap: 0.64418 time: 48.61561\r","output_type":"stream"}]},{"cell_type":"markdown","source":"# Thank you for reading my kernel!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}